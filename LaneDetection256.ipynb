{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b70271-e574-4ddc-b34d-b14d99a3e5cb",
   "metadata": {},
   "source": [
    "# Simple diffusion\n",
    "This is a simple diffusion model heavily based on [minDiffusion](https://github.com/cloneofsimo/minDiffusion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddbc4462-c498-418a-9f71-ed5ef19d545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "                        import os, random, time, math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5857a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4\n",
    "MODEL_OUT_DIR = r\"./models\"\n",
    "SAMPLE_OUT_DIR = r\"./samples\"\n",
    "\n",
    "# Diffusion\n",
    "NOISE_STEPS = 200\n",
    "BETA_START = 1e-4\n",
    "BETA_END = 2e-2\n",
    "\n",
    "# Training\n",
    "INIT_LR = 1e-5\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath).convert('RGB')\n",
    "    img = img.resize((256, 256), Image.BICUBIC)\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_img(image_tensor, filename):\n",
    "    \n",
    "    #image_numpy = image_tensor.detach().cpu().numpy()\n",
    "    image_numpy = image_tensor[0].cpu().detach()\n",
    "    image_numpy = image_numpy.numpy()\n",
    "    print(\"shape of numpy_image\",image_numpy.shape)\n",
    "    imput_numpy = image_numpy.astype(np.float)\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    " \n",
    "    #image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
    "    image_numpy = image_numpy.clip(0, 255)\n",
    "    #image_pil = Image.fromarray(np.uint8(255 * image_numpy))\n",
    "    image_numpy = image_numpy.astype(np.uint8)\n",
    "    image_pil = Image.fromarray(image_numpy)\n",
    "    image_pil.save(filename)\n",
    "    #cv2.imwrite(filename, image_pil)\n",
    "    print(\"Image saved as {}\".format(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f7f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, direction):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.direction = direction\n",
    "        self.a_path = join(image_dir, \"a\")\n",
    "        self.b_path = join(image_dir, \"b\")\n",
    "        \n",
    "        self.image_filenames = [x for x in listdir(self.a_path) if is_image_file(x)]\n",
    "        \n",
    "        \n",
    "\n",
    "        transform_list = [transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print(join(self.b_path, self.image_filenames[index]))\n",
    "        #print(join(self.a_path, self.image_filenames[index]))\n",
    "        #print('//')\n",
    "        a = Image.open(join(self.a_path, self.image_filenames[index]))#.convert('RGB')\n",
    "        \n",
    "        b = Image.open(join(self.b_path, self.image_filenames[index]).replace(\"png\",\"jpg\")).convert('RGB')\n",
    "        \n",
    "        a = a.resize((256, 256), Image.BICUBIC)\n",
    "        b = b.resize((256, 256), Image.BICUBIC)\n",
    "        a = transforms.ToTensor()(a)\n",
    "        b = transforms.ToTensor()(b)\n",
    "\n",
    "    \n",
    "        a = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(a)\n",
    "        b = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(b)\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            idx = [i for i in range(a.size(2) - 1, -1, -1)]\n",
    "            idx = torch.LongTensor(idx)\n",
    "            a = a.index_select(2, idx)\n",
    "            b = b.index_select(2, idx)\n",
    "\n",
    "        if self.direction == \"a2b\":\n",
    "            return a, b\n",
    "        else:\n",
    "            return b, a\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621c5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "\n",
    "def get_training_set(root_dir, direction):\n",
    "    train_dir = join(root_dir, \"train\")\n",
    "\n",
    "    return DatasetFromFolder(train_dir, direction)\n",
    "\n",
    "\n",
    "def get_test_set(root_dir, direction):\n",
    "    test_dir = join(root_dir, \"test\")\n",
    "\n",
    "    return DatasetFromFolder(test_dir, direction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1a93a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhhhh\n",
      "<__main__.DatasetFromFolder object at 0x70290af23890>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "root_path = \"./data/\"\n",
    "print('hhhhh')\n",
    "train_set = get_training_set(root_path, 'b2a')\n",
    "print(train_set)\n",
    "test_set = get_test_set(root_path, 'b2a')\n",
    "train_data_loader= DataLoader(dataset=train_set,  batch_size=BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "testing_data_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90651f4-4992-4fc5-83d4-1c2232a8cc2a",
   "metadata": {},
   "source": [
    "## Model\n",
    "The denoising model is a simple U-Net structure based on [minDiffusion](https://github.com/cloneofsimo/minDiffusion). It uses sinusoidal position embeddings to encode time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab8b654c-46f5-4a38-8852-4c0e3abb529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"Taken verbatim from https://huggingface.co/blog/annotated-diffusion\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9b1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "from skimage import util\n",
    "import torch.nn.functional as FF\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "\n",
    "def create_dir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def same_padding(images, ksizes, strides, rates):   \n",
    "    assert len(images.size()) == 4\n",
    "    batch_size, channel, rows, cols = images.size()\n",
    "    out_rows = (rows + strides[0] - 1) // strides[0]\n",
    "    out_cols = (cols + strides[1] - 1) // strides[1]\n",
    "    effective_k_row = (ksizes[0] - 1) * rates[0] + 1\n",
    "    effective_k_col = (ksizes[1] - 1) * rates[1] + 1\n",
    "    padding_rows = max(0, (out_rows-1)*strides[0]+effective_k_row-rows)\n",
    "    padding_cols = max(0, (out_cols-1)*strides[1]+effective_k_col-cols)\n",
    "    # Pad the input\n",
    "    padding_top = int(padding_rows / 2.)\n",
    "    padding_left = int(padding_cols / 2.)\n",
    "    padding_bottom = padding_rows - padding_top\n",
    "    padding_right = padding_cols - padding_left\n",
    "    paddings = (padding_left, padding_right, padding_top, padding_bottom)\n",
    "    images = torch.nn.ZeroPad2d(paddings)(images)\n",
    "    return images\n",
    "\n",
    "def reduce_mean(x, axis=None, keepdim=False):\n",
    "    if not axis:\n",
    "        axis = range(len(x.shape))\n",
    "    for i in sorted(axis, reverse=True):\n",
    "        x = torch.mean(x, dim=i, keepdim=keepdim)\n",
    "    return x\n",
    "\n",
    "def reduce_sum(x, axis=None, keepdim=False):\n",
    "    if not axis:\n",
    "        axis = range(len(x.shape))\n",
    "    for i in sorted(axis, reverse=True):\n",
    "        x = torch.sum(x, dim=i, keepdim=keepdim)\n",
    "    return x\n",
    "\n",
    "def extract_image_patches(images, ksizes, strides, padding='same'):\n",
    "\n",
    "    assert len(images.size()) == 4\n",
    "    assert padding in ['same', 'valid']\n",
    "    batch_size, channel, height, width = images.size()\n",
    "\n",
    "    if padding == 'same':\n",
    "        images = same_padding(images, ksizes, strides, [1, 1])\n",
    "    elif padding == 'valid':\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError('Unsupported padding type: {}.\\\n",
    "            Only \"same\" or \"valid\" are supported.'.format(padding))\n",
    "    batch_size, channel, height, width = images.size()\n",
    "\n",
    "    unfold = torch.nn.Unfold(kernel_size=ksizes,\n",
    "                            padding=0,\n",
    "                            stride=strides)\n",
    "    patches = unfold(images)\n",
    "    return patches\n",
    "\n",
    "def np_free_form_mask(h, w, maxVertex, maxLength, maxBrushWidth, maxAngle):\n",
    "\n",
    "    mask = np.zeros((h, w), np.float32)\n",
    "    numVertex = np.random.randint(maxVertex + 1)\n",
    "    startY = np.random.randint(h)\n",
    "    startX = np.random.randint(w)\n",
    "    brushWidth = 0\n",
    "    for i in range(numVertex):\n",
    "        angle = np.random.randint(maxAngle + 1)\n",
    "        angle = angle / 360.0 * 2 * np.pi\n",
    "        if i % 2 == 0:\n",
    "            angle = 2 * np.pi - angle\n",
    "        length = np.random.randint(maxLength + 1)\n",
    "        brushWidth = np.random.randint(10, maxBrushWidth + 1) // 2 * 2\n",
    "        nextY = startY + length * np.cos(angle)\n",
    "        nextX = startX + length * np.sin(angle)\n",
    "\n",
    "        nextY = np.maximum(np.minimum(nextY, h - 1), 0).astype(np.int)\n",
    "        nextX = np.maximum(np.minimum(nextX, w - 1), 0).astype(np.int)\n",
    "\n",
    "        cv2.line(mask, (startY, startX), (nextY, nextX), 1, brushWidth)\n",
    "        cv2.circle(mask, (startY, startX), brushWidth // 2, 2)\n",
    "\n",
    "        startY, startX = nextY, nextX\n",
    "    cv2.circle(mask, (startY, startX), brushWidth // 2, 2)\n",
    "    return mask\n",
    "\n",
    "def free_form_mask(h, w, parts=8, maxVertex=16, maxLength=80, maxBrushWidth=20, maxAngle=360):\n",
    "    mask = np.zeros((h, w), np.float32)\n",
    "    for i in range(parts):\n",
    "        p = np_free_form_mask(h, w, maxVertex, maxLength, maxBrushWidth, maxAngle)\n",
    "        mask = mask + p\n",
    "    mask = np.minimum(mask, 1.0)\n",
    "    return mask\n",
    "\n",
    "def generate_mask_stroke(im_size, parts=16, maxVertex=24, maxLength=100, maxBrushWidth=24, maxAngle=360):\n",
    "    h, w = im_size[:2]\n",
    "    mask = np.zeros((h, w, 1), dtype=np.float32)\n",
    "    for i in range(parts):\n",
    "        mask = mask + np_free_form_mask( h, w, maxVertex, maxLength, maxBrushWidth, maxAngle)\n",
    "    mask = np.minimum(mask, 1.0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def generate_noise(image, noise_type=\"gauss\"):\n",
    "    if noise_type == \"gauss\":\n",
    "        noise = np.random.normal(0.0, 50/255.0, image.shape)\n",
    "        out = noise+image\n",
    "\n",
    "    if noise_type == \"salt\":\n",
    "        out = util.random_noise(image=image, mode='salt', clip=True, amount=0.2)\n",
    "\n",
    "    if noise_type == \"poisson\":\n",
    "        vals = len(np.unique(image))\n",
    "        vals = 2 ** np.ceil(np.log2(vals))\n",
    "        out = np.random.poisson(image * vals) / float(vals)\n",
    "\n",
    "    if noise_type == \"speckle\":\n",
    "        row,col,ch = image.shape\n",
    "        gauss = np.random.randn(row,col,ch)\n",
    "        gauss = gauss.reshape(row,col,ch)        \n",
    "        out = image + image * gauss\n",
    "\n",
    "    if noise_type == \"s&p\":\n",
    "        out = util.random_noise(image=image, mode='s&p', clip=True, amount=0.2, salt_vs_pepper=0.5)\n",
    "\n",
    "    return np.uint8(noise)\n",
    "\n",
    "def generate_rectangle(h, w):\n",
    "    mask = np.ones((h, w))\n",
    "    crop_size = h//2\n",
    "    startY = np.random.randint(0, h-crop_size)\n",
    "    startX = np.random.randint(0, w-crop_size)\n",
    "    mask[startY: startY+crop_size, startX: startX+crop_size] = 0\n",
    "    return mask \n",
    "    \n",
    "def generate_graffiti(h, w, noise):\n",
    "    mask = np.ones((h, w))\n",
    "    idx1 = noise[:, :, 0] == 0\n",
    "    idx2 = noise[:, :, 1] == 0                \n",
    "    idx3 = noise[:, :, 2] == 0\n",
    "    idx = idx1 == idx2\n",
    "    idx = idx == idx3\n",
    "    mask[idx] = 0\n",
    "    return mask\n",
    "\n",
    "def stitch_images(inputs, *outputs, img_per_row=2):\n",
    "    gap = 5\n",
    "    columns = len(outputs) + 1\n",
    "\n",
    "    height, width = inputs[0][:, :, 0].shape\n",
    "    img = Image.new('RGB', (width * img_per_row * columns + gap * (img_per_row - 1), height * int(len(inputs) / img_per_row)))\n",
    "    images = [inputs, *outputs]\n",
    "\n",
    "    for ix in range(len(inputs)):\n",
    "        xoffset = int(ix % img_per_row) * width * columns + int(ix % img_per_row) * gap\n",
    "        yoffset = int(ix / img_per_row) * height\n",
    "\n",
    "        for cat in range(len(images)):\n",
    "            im = np.array((images[cat][ix]).cpu()).astype(np.uint8).squeeze()\n",
    "            im = Image.fromarray(im)\n",
    "            img.paste(im, (xoffset + cat * width, yoffset))\n",
    "\n",
    "    return img\n",
    "\n",
    "# def random_crop(npdata, crop_size, datatype):\n",
    "    \n",
    "#     height, width = npdata.shape[0:2]\n",
    "#     mask = np.ones((height, width))\n",
    "\n",
    "#     if datatype == 1:\n",
    "#         h = random.randint(0, height - crop_size)\n",
    "#         w = random.randint(0, width - crop_size)\n",
    "#         mask[h: h+crop_size, w: w+crop_size] = 0\n",
    "#         crop_image = npdata[h: h+crop_size, w: w+crop_size]\n",
    "    \n",
    "#     if datatype == 2:\n",
    "#         h = 0\n",
    "#         w = random.randint(0, width - crop_size)\n",
    "#         mask[:, w: w+crop_size] = 0\n",
    "#         crop_image = npdata[:, w: w+crop_size] \n",
    "#     return crop_image, (w, h), mask\n",
    "\n",
    "    \n",
    "def gauss_kernel(size=21, sigma=3):\n",
    "    interval = (2 * sigma + 1.0) / size\n",
    "    x = np.linspace(-sigma-interval/2, sigma+interval/2, size+1)\n",
    "    ker1d = np.diff(st.norm.cdf(x))\n",
    "    kernel_raw = np.sqrt(np.outer(ker1d, ker1d))\n",
    "    kernel = kernel_raw / kernel_raw.sum()\n",
    "    out_filter = np.array(kernel, dtype=np.float32)\n",
    "    out_filter = out_filter.reshape((size, size, 1, 1))\n",
    "    return out_filter\n",
    "\n",
    "def same_padding(images, ksizes, strides, rates):\n",
    "    assert len(images.size()) == 4\n",
    "    batch_size, channel, rows, cols = images.size()\n",
    "    out_rows = (rows + strides[0] - 1) // strides[0]\n",
    "    out_cols = (cols + strides[1] - 1) // strides[1]\n",
    "    effective_k_row = (ksizes[0] - 1) * rates[0] + 1\n",
    "    effective_k_col = (ksizes[1] - 1) * rates[1] + 1\n",
    "    padding_rows = max(0, (out_rows-1)*strides[0]+effective_k_row-rows)\n",
    "    padding_cols = max(0, (out_cols-1)*strides[1]+effective_k_col-cols)\n",
    "    # Pad the input\n",
    "    padding_top = int(padding_rows / 2.)\n",
    "    padding_left = int(padding_cols / 2.)\n",
    "    padding_bottom = padding_rows - padding_top\n",
    "    padding_right = padding_cols - padding_left\n",
    "    paddings = (padding_left, padding_right, padding_top, padding_bottom)\n",
    "    images = torch.nn.ZeroPad2d(paddings)(images)\n",
    "    return images\n",
    "\n",
    "def random_crop(npdata, crop_size, datatype, count, pos, known_mask=None):\n",
    "    \n",
    "    height, width = npdata.shape[0:2]\n",
    "    mask = np.ones((height, width))\n",
    "\n",
    "    if datatype == 1:\n",
    "        if count == 0 and not known_mask:\n",
    "            h = random.randint(0, height - crop_size)\n",
    "            w = random.randint(0, width - crop_size)\n",
    "        else:\n",
    "            w, h = pos[0], pos[1]\n",
    "        mask[h: h+crop_size, w: w+crop_size] = 0\n",
    "        crop_image = npdata[h: h+crop_size, w: w+crop_size]\n",
    "    \n",
    "    if datatype == 2:\n",
    "        h = 0\n",
    "        w = random.randint(0, width - crop_size)\n",
    "        mask[:, w: w+crop_size] = 0\n",
    "        crop_image = npdata[:, w: w+crop_size] \n",
    "    return crop_image, (w, h), mask\n",
    "\n",
    "def center_crop(npdata, crop_size):\n",
    "    height, width = npdata.shape[0:2]\n",
    "    mask = np.ones((height, width))\n",
    "    w = 64\n",
    "    h = 64\n",
    "    mask[h: h+crop_size, w: w+crop_size] = 0\n",
    "\n",
    "    crop_image = npdata[h: h+crop_size, w: w+crop_size]\n",
    "    return crop_image, (w, h), mask\n",
    "    \n",
    "def side_crop(data, crop_size):\n",
    "    height, width = data.shape[0:2]\n",
    "    mask = np.ones((height, width))\n",
    "    \n",
    "    w = (width - crop_size) // 2\n",
    "    h = 0\n",
    "    mask[:, 0: w] = 0.\n",
    "    mask[:, w+crop_size:] = 0.\n",
    "    \n",
    "    return (w, h), mask\n",
    "\n",
    "def imshow(img, title=''):\n",
    "    fig = plt.gcf()\n",
    "    fig.canvas.set_window_title(title)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, interpolation='none')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def imsave(img, path):\n",
    "    im = Image.fromarray(img.cpu().numpy().astype(np.uint8).squeeze())\n",
    "    im.save(path)\n",
    "\n",
    "def savetxt(arr, path):\n",
    "    np.savetxt(path, arr.cpu().numpy().squeeze(), fmt='%.2f')\n",
    "    \n",
    "def template_match(target, source):\n",
    "    locs = []\n",
    "    _src = []\n",
    "    for i in range(target.shape[0]):\n",
    "        src = source[i].detach().cpu().permute(1, 2, 0).numpy()\n",
    "        tar = target[i].detach().cpu().permute(1, 2, 0).numpy()\n",
    "        \n",
    "        src_gray = cv2.cvtColor(src, cv2.COLOR_RGB2GRAY)\n",
    "        tar_gray = cv2.cvtColor(tar, cv2.COLOR_RGB2GRAY)\n",
    "        w, h = tar_gray.shape[::-1]\n",
    "\n",
    "        res = cv2.matchTemplate(src_gray, tar_gray, cv2.TM_CCOEFF)\n",
    "        min_val, max_val, min_loc, loc = cv2.minMaxLoc(res)\n",
    "        locs.append(loc)\n",
    "        \n",
    "        src = src * 255\n",
    "        im = Image.fromarray(src.astype(np.uint8).squeeze())\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        draw.rectangle([loc, (loc[0] + w, loc[1] + h)], outline=0)\n",
    "        im = np.array(im)\n",
    "        _src.append(im)\n",
    "        \n",
    "    return torch.Tensor(_src), locs    \n",
    "\n",
    "\n",
    "def make_mask(data, pdata, pos, device):\n",
    "    \n",
    "    crop_size = pdata.shape[3]\n",
    "    mask_with_pdata = torch.zeros(data.shape).to(device)\n",
    "    mask_with_ones = torch.ones(data.shape).to(device)\n",
    "\n",
    "    for po in range(len(pos)):\n",
    "        w, h = pos[po][0], pos[po][1]\n",
    "        mask_with_pdata[po, :, h: h+crop_size, w: w+crop_size] = pdata[po]\n",
    "        mask_with_ones[po, :, h: h+crop_size, w: w+crop_size] = 0\n",
    "\n",
    "    return mask_with_pdata, mask_with_ones\n",
    "    \n",
    "\n",
    "class Progbar(object):\n",
    "    \"\"\"Displays a progress bar.\n",
    "\n",
    "    Arguments:\n",
    "        target: Total number of steps expected, None if unknown.\n",
    "        width: Progress bar width on screen.\n",
    "        verbose: Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)\n",
    "        stateful_metrics: Iterable of string names of metrics that\n",
    "            should *not* be averaged over time. Metrics in this list\n",
    "            will be displayed as-is. All others will be averaged\n",
    "            by the progbar before display.\n",
    "        interval: Minimum visual progress update interval (in seconds).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, width=25, verbose=1, interval=0.05,\n",
    "                 stateful_metrics=None):\n",
    "        self.target = target\n",
    "        self.width = width\n",
    "        self.verbose = verbose\n",
    "        self.interval = interval\n",
    "        if stateful_metrics:\n",
    "            self.stateful_metrics = set(stateful_metrics)\n",
    "        else:\n",
    "            self.stateful_metrics = set()\n",
    "\n",
    "        self._dynamic_display = ((hasattr(sys.stdout, 'isatty') and\n",
    "                                  sys.stdout.isatty()) or\n",
    "                                 'ipykernel' in sys.modules or\n",
    "                                 'posix' in sys.modules)\n",
    "        self._total_width = 0\n",
    "        self._seen_so_far = 0\n",
    "        # We use a dict + list to avoid garbage collection\n",
    "        # issues found in OrderedDict\n",
    "        self._values = {}\n",
    "        self._values_order = []\n",
    "        self._start = time.time()\n",
    "        self._last_update = 0\n",
    "\n",
    "    def update(self, current, values=None):\n",
    "        \"\"\"Updates the progress bar.\n",
    "\n",
    "        Arguments:\n",
    "            current: Index of current step.\n",
    "            values: List of tuples:\n",
    "                `(name, value_for_last_step)`.\n",
    "                If `name` is in `stateful_metrics`,\n",
    "                `value_for_last_step` will be displayed as-is.\n",
    "                Else, an average of the metric over time will be displayed.\n",
    "        \"\"\"\n",
    "        values = values or []\n",
    "        for k, v in values:\n",
    "            if k not in self._values_order:\n",
    "                self._values_order.append(k)\n",
    "            if k not in self.stateful_metrics:\n",
    "                if k not in self._values:\n",
    "                    self._values[k] = [v * (current - self._seen_so_far),\n",
    "                                       current - self._seen_so_far]\n",
    "                else:\n",
    "                    self._values[k][0] += v * (current - self._seen_so_far)\n",
    "                    self._values[k][1] += (current - self._seen_so_far)\n",
    "            else:\n",
    "                self._values[k] = v\n",
    "        self._seen_so_far = current\n",
    "\n",
    "        now = time.time()\n",
    "        info = ' - %.0fs' % (now - self._start)\n",
    "        if self.verbose == 1:\n",
    "            if (now - self._last_update < self.interval and\n",
    "                    self.target is not None and current < self.target):\n",
    "                return\n",
    "\n",
    "            prev_total_width = self._total_width\n",
    "            if self._dynamic_display:\n",
    "                sys.stdout.write('\\b' * prev_total_width)\n",
    "                sys.stdout.write('\\r')\n",
    "            else:\n",
    "                sys.stdout.write('\\n')\n",
    "\n",
    "            if self.target is not None:\n",
    "                numdigits = int(np.floor(np.log10(self.target))) + 1\n",
    "                barstr = '%%%dd/%d [' % (numdigits, self.target)\n",
    "                bar = barstr % current\n",
    "                prog = float(current) / self.target\n",
    "                prog_width = int(self.width * prog)\n",
    "                if prog_width > 0:\n",
    "                    bar += ('=' * (prog_width - 1))\n",
    "                    if current < self.target:\n",
    "                        bar += '>'\n",
    "                    else:\n",
    "                        bar += '='\n",
    "                bar += ('.' * (self.width - prog_width))\n",
    "                bar += ']'\n",
    "            else:\n",
    "                bar = '%7d/Unknown' % current\n",
    "\n",
    "            self._total_width = len(bar)\n",
    "            sys.stdout.write(bar)\n",
    "\n",
    "            if current:\n",
    "                time_per_unit = (now - self._start) / current\n",
    "            else:\n",
    "                time_per_unit = 0\n",
    "            if self.target is not None and current < self.target:\n",
    "                eta = time_per_unit * (self.target - current)\n",
    "                if eta > 3600:\n",
    "                    eta_format = '%d:%02d:%02d' % (eta // 3600,\n",
    "                                                   (eta % 3600) // 60,\n",
    "                                                   eta % 60)\n",
    "                elif eta > 60:\n",
    "                    eta_format = '%d:%02d' % (eta // 60, eta % 60)\n",
    "                else:\n",
    "                    eta_format = '%ds' % eta\n",
    "\n",
    "                info = ' - ETA: %s' % eta_format\n",
    "            else:\n",
    "                if time_per_unit >= 1:\n",
    "                    info += ' %.0fs/step' % time_per_unit\n",
    "                elif time_per_unit >= 1e-3:\n",
    "                    info += ' %.0fms/step' % (time_per_unit * 1e3)\n",
    "                else:\n",
    "                    info += ' %.0fus/step' % (time_per_unit * 1e6)\n",
    "\n",
    "            for k in self._values_order:\n",
    "                info += ' - %s:' % k\n",
    "                if isinstance(self._values[k], list):\n",
    "                    avg = np.mean(self._values[k][0] / max(1, self._values[k][1]))\n",
    "                    if abs(avg) > 1e-3:\n",
    "                        info += ' %.4f' % avg\n",
    "                    else:\n",
    "                        info += ' %.4e' % avg\n",
    "                else:\n",
    "                    info += ' %s' % self._values[k]\n",
    "\n",
    "            self._total_width += len(info)\n",
    "            if prev_total_width > self._total_width:\n",
    "                info += (' ' * (prev_total_width - self._total_width))\n",
    "\n",
    "            if self.target is not None and current >= self.target:\n",
    "                info += '\\n'\n",
    "\n",
    "            sys.stdout.write(info)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        elif self.verbose == 2:\n",
    "            if self.target is None or current >= self.target:\n",
    "                for k in self._values_order:\n",
    "                    info += ' - %s:' % k\n",
    "                    avg = np.mean(self._values[k][0] / max(1, self._values[k][1]))\n",
    "                    if avg > 1e-3:\n",
    "                        info += ' %.4f' % avg\n",
    "                    else:\n",
    "                        info += ' %.4e' % avg\n",
    "                info += '\\n'\n",
    "\n",
    "                sys.stdout.write(info)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        self._last_update = now\n",
    "\n",
    "    def add(self, n, values=None):\n",
    "        self.update(self._seen_so_far + n, values)\n",
    "\n",
    "import math\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "class Adam16(Optimizer):\n",
    "  def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0):\n",
    "    \n",
    "    defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "            weight_decay=weight_decay)\n",
    "    params = list(params)\n",
    "    super(Adam16, self).__init__(params, defaults)\n",
    "      \n",
    "  # Safety modification to make sure we floatify our state\n",
    "  def load_state_dict(self, state_dict):\n",
    "    super(Adam16, self).load_state_dict(state_dict)\n",
    "    for group in self.param_groups:\n",
    "      for p in group['params']:\n",
    "        \n",
    "        self.state[p]['exp_avg'] = self.state[p]['exp_avg'].float()\n",
    "        self.state[p]['exp_avg_sq'] = self.state[p]['exp_avg_sq'].float()\n",
    "        self.state[p]['fp32_p'] = self.state[p]['fp32_p'].float()\n",
    "\n",
    "  def step(self, closure=None):\n",
    "    \"\"\"Performs a single optimization step.\n",
    "    Arguments:\n",
    "      closure (callable, optional): A closure that reevaluates the model\n",
    "        and returns the loss.\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    if closure is not None:\n",
    "      loss = closure()\n",
    "\n",
    "    for group in self.param_groups:\n",
    "      for p in group['params']:\n",
    "        if p.grad is None:\n",
    "          continue\n",
    "          \n",
    "        grad = p.grad.data.float()\n",
    "        state = self.state[p]\n",
    "\n",
    "        # State initialization\n",
    "        if len(state) == 0:\n",
    "          state['step'] = 0\n",
    "          # Exponential moving average of gradient values\n",
    "          state['exp_avg'] = grad.new().resize_as_(grad).zero_()\n",
    "          # Exponential moving average of squared gradient values\n",
    "          state['exp_avg_sq'] = grad.new().resize_as_(grad).zero_()\n",
    "          # Fp32 copy of the weights\n",
    "          state['fp32_p'] = p.data.float()\n",
    "\n",
    "        exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "        beta1, beta2 = group['betas']\n",
    "\n",
    "        state['step'] += 1\n",
    "\n",
    "        if group['weight_decay'] != 0:\n",
    "          grad = grad.add(group['weight_decay'], state['fp32_p'])\n",
    "\n",
    "        # Decay the first and second moment running average coefficient\n",
    "        exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "        exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "        denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "        bias_correction1 = 1 - beta1 ** state['step']\n",
    "        bias_correction2 = 1 - beta2 ** state['step']\n",
    "        step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "      \n",
    "        state['fp32_p'].addcdiv_(-step_size, exp_avg, denom)\n",
    "        p.data = state['fp32_p'].float()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def cus_sample(feat, **kwargs):\n",
    "    \"\"\"\n",
    "    :param feat: 输入特征\n",
    "    :param kwargs: size或者scale_factor\n",
    "    \"\"\"\n",
    "    assert len(kwargs.keys()) == 1 and list(kwargs.keys())[0] in [\"size\", \"scale_factor\"]\n",
    "    return FF.interpolate(feat, **kwargs, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "\n",
    "def upsample_add(*xs):\n",
    "    y = xs[-1]\n",
    "    for x in xs[:-1]:\n",
    "        y = y + FF.interpolate(x, size=y.size()[2:], mode=\"bilinear\", align_corners=False)\n",
    "    return y\n",
    "\n",
    "\n",
    "def extract_image_patches(images, ksizes, strides, rates, padding='same'):\n",
    "    \"\"\"\n",
    "    Extract patches from images and put them in the C output dimension.\n",
    "    :param padding:\n",
    "    :param images: [batch, channels, in_rows, in_cols]. A 4-D Tensor with shape\n",
    "    :param ksizes: [ksize_rows, ksize_cols]. The size of the sliding window for\n",
    "     each dimension of images\n",
    "    :param strides: [stride_rows, stride_cols]\n",
    "    :param rates: [dilation_rows, dilation_cols]\n",
    "    :return: A Tensor\n",
    "    \"\"\"\n",
    "    assert len(images.size()) == 4\n",
    "    assert padding in ['same', 'valid']\n",
    "    batch_size, channel, height, width = images.size()\n",
    "\n",
    "    if padding == 'same':\n",
    "        images = same_padding(images, ksizes, strides, rates)\n",
    "    elif padding == 'valid':\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError('Unsupported padding type: {}.\\\n",
    "                Only \"same\" or \"valid\" are supported.'.format(padding))\n",
    "\n",
    "    unfold = torch.nn.Unfold(kernel_size=ksizes,\n",
    "                             dilation=rates,\n",
    "                             padding=0,\n",
    "                             stride=strides)\n",
    "    patches = unfold(images)\n",
    "    return patches  # [N, C*k*k, L], L is the total number of such blocks\n",
    "\n",
    "\n",
    "\n",
    "def PositionEmbeddingSine(opt):\n",
    "    temperature=10000\n",
    "    feature_h = opt.crop_size//2**opt.n_downsample\n",
    "    num_pos_feats = opt.ngf*(2**opt.n_downsample) // 2\n",
    "    mask = torch.ones((feature_h, feature_h))\n",
    "    y_embed = mask.cumsum(0, dtype=torch.float32)\n",
    "    x_embed = mask.cumsum(1, dtype=torch.float32)\n",
    "    # if self.normalize:\n",
    "    #     eps = 1e-6\n",
    "    #     y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "    #     x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "\n",
    "    dim_t = torch.arange(num_pos_feats, dtype=torch.float32)\n",
    "    dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\n",
    "\n",
    "    pos_x = x_embed[:, :, None] / dim_t\n",
    "    pos_y = y_embed[:, :, None] / dim_t\n",
    "    pos_x = torch.stack((pos_x[:, :, 0::2].sin(), pos_x[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "    pos_y = torch.stack((pos_y[:, :, 0::2].sin(), pos_y[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "    pos = torch.cat((pos_y, pos_x), dim=2).permute(2, 0, 1)\n",
    "    return pos\n",
    "\n",
    "\n",
    "def PatchPositionEmbeddingSine(ksize, stride):\n",
    "    temperature=10000\n",
    "    feature_h = int((256-ksize)/stride)+1\n",
    "    num_pos_feats = 256//2\n",
    "    mask = torch.ones((feature_h, feature_h))\n",
    "    y_embed = mask.cumsum(0, dtype=torch.float32)\n",
    "    x_embed = mask.cumsum(1, dtype=torch.float32)\n",
    "    # if self.normalize:\n",
    "    #     eps = 1e-6\n",
    "    #     y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "    #     x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "\n",
    "    dim_t = torch.arange(num_pos_feats, dtype=torch.float32)\n",
    "    dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\n",
    "\n",
    "    pos_x = x_embed[:, :, None] / dim_t\n",
    "    pos_y = y_embed[:, :, None] / dim_t\n",
    "    pos_x = torch.stack((pos_x[:, :, 0::2].sin(), pos_x[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "    pos_y = torch.stack((pos_y[:, :, 0::2].sin(), pos_y[:, :, 1::2].cos()), dim=3).flatten(2)\n",
    "    pos = torch.cat((pos_y, pos_x), dim=2).permute(2, 0, 1)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd8275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "class TransformerEncoders(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6, dim_feedforward=2048, dropout=0.0,\n",
    "                 activation=\"relu\", normalize_before=False,\n",
    "                 return_intermediate_dec=False, withCIA=False):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout, activation, normalize_before, withCIA)\n",
    "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, src_key_padding_mask=None, src_pos=None):\n",
    "        memory = self.encoder(src, pos=src_pos, src_key_padding_mask=src_key_padding_mask)\n",
    "        return memory\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None, withCIA=None):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src,\n",
    "                mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None):\n",
    "        output = src\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            output = layer(output, src_mask=mask,\n",
    "                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False, withCIA=False):\n",
    "        super().__init__()\n",
    "        self.global_token_mixer = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.local_token_mixer = Local_Token_Mixer(dim=d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.accelerator = CIA()\n",
    "\n",
    "        self.normalize_before = normalize_before\n",
    "        self.withCIA = withCIA\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_post(self,\n",
    "                     src,\n",
    "                     src_mask: Optional[Tensor] = None,\n",
    "                     src_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None):\n",
    "        q = k = self.with_pos_embed(src, pos)\n",
    "        src2 = self.global_token_mixer(q, k, value=src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        if self.withCIA == True:\n",
    "            src2 = self.accelerator(src, src2)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.local_token_mixer(src)\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "    def forward_pre(self, src,\n",
    "                    src_mask: Optional[Tensor] = None,\n",
    "                    src_key_padding_mask: Optional[Tensor] = None,\n",
    "                    pos: Optional[Tensor] = None):\n",
    "        src2 = self.norm1(src)\n",
    "        q = k = self.with_pos_embed(src2, pos)\n",
    "        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        return src\n",
    "\n",
    "    def forward(self, src,\n",
    "                src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None):\n",
    "        if self.normalize_before:\n",
    "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
    "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
    "\n",
    "\n",
    "class CIA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "        \n",
    "    def forward(self, x_pre, x_pos):\n",
    "        [N, B, C] = x_pre.shape\n",
    "        fea_pred = F.normalize(x_pre, dim=2)\n",
    "        fea_later = F.normalize(x_pos, dim=2)\n",
    "        dis = torch.bmm(fea_pred.permute(1, 0, 2), fea_later.permute(1, 2, 0))\n",
    "        dis = torch.diagonal(dis, dim1=1, dim2=2).unsqueeze(-1)\n",
    "        weight = self.sigmoid(dis)\n",
    "        weight = 1 - weight\n",
    "        out = x_pos * weight.unsqueeze(1).reshape(N, B, 1)\n",
    "        return out\n",
    " \n",
    "\n",
    "class Local_Token_Mixer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.CTI = Cross_correlation_Token_Interaction(dim=dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.CTI(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Cross_correlation_Token_Interaction(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.softmax = nn.Softmax(dim=-1) \n",
    "        self.linear = nn.Linear(dim, 2, bias=False)\n",
    "        self.depthwise = Depthwise_Conv(dim)\n",
    "    \n",
    "    def interaction(self, x):\n",
    "        [B, C, N] = x.shape\n",
    "        q = k = x\n",
    "        matmul = torch.bmm(q.permute(0, 2, 1), k) # transpose check\n",
    "        q_abs = torch.sqrt(torch.sum(q.pow(2) + 1e-6, dim=1, keepdim=True))\n",
    "        k_abs = torch.sqrt(torch.sum(k.pow(2) + 1e-6, dim=1, keepdim=True))\n",
    "        abs_matmul = torch.bmm(q_abs.permute(0, 2, 1), k_abs)\n",
    "        io_abs = matmul / abs_matmul\n",
    "\n",
    "        corr_seq = torch.zeros(x.shape).cuda()\n",
    "        \n",
    "        for i in range(B):\n",
    "\n",
    "            abs = io_abs[i].fill_diagonal_(0)\n",
    "            _map=torch.argmax(abs, dim=1)\n",
    "\n",
    "            corr_seq[i, :, :] = x[i, :, _map]\n",
    "        \n",
    "        fus = x + corr_seq\n",
    "        fus = fus.permute(0, 2, 1)\n",
    "        weight = self.linear(fus)\n",
    "        weight = self.softmax(weight)\n",
    "        weight = weight.permute(0, 2, 1)\n",
    "        output = x * weight[:, 0:1, :] + corr_seq * weight[:, 1:2, :]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def token_partition(self, x, local_size):\n",
    "\n",
    "        [N, B, C] = x.shape\n",
    "        x = x.view(N // local_size, local_size, B, C)\n",
    "        local = x.permute(1, 0, 2, 3).reshape(local_size, -1, C).permute(1, 2, 0)\n",
    "        return local\n",
    "\n",
    "    def token_reverse(self, x, local_size, token_size):\n",
    "        B = int(x.shape[0] / (token_size / local_size))\n",
    "        x = x.view(B, token_size // local_size, -1, local_size)\n",
    "        output = x.reshape(B, -1, token_size)\n",
    "        return output     \n",
    "            \n",
    "    def forward(self, x):\n",
    "        local_seq = self.token_partition(x, local_size=1024)\n",
    "        intered_seq = self.interaction(local_seq)\n",
    "        s_intered_seq = self.token_reverse(intered_seq, local_size=1024, token_size=4096)\n",
    "        output = self.depthwise(s_intered_seq)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Depthwise_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, bias=False, kernel_size=7, padding=3):\n",
    "        super().__init__()\n",
    "        med_channels = int(dim * 2)\n",
    "        self.dwconv = nn.Conv2d(\n",
    "                    dim, dim, kernel_size=kernel_size,\n",
    "                    padding=padding, groups=dim, bias=bias) \n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, med_channels, bias=bias)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(med_channels, dim, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        [B, C, N] = x.shape\n",
    "        x = x.view(B, C, 64, 64)\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(2, 3, 0, 1).reshape(N, B, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pwconv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c65f5966-a3d5-4b6b-98d2-6cac33aa62f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch.nn.utils.spectral_norm as spectral_norm\n",
    "import math\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.utils import _single, _pair, _triple\n",
    "#from . import transformer\n",
    "#import PatchPositionEmbeddingSine\n",
    "\n",
    "class BaseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNetwork, self).__init__()\n",
    "\n",
    "    def init_weights(self, init_type='xavier', gain=0.02):\n",
    "        def init_func(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "                if init_type == 'normal':\n",
    "                    nn.init.normal_(m.weight.data, 0.0, gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "                elif init_type == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    nn.init.orthogonal_(m.weight.data, gain=gain)\n",
    "\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "            elif classname.find('BatchNorm2d') != -1:\n",
    "                nn.init.normal_(m.weight.data, 1.0, gain)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        self.apply(init_func)\n",
    "\n",
    "class FFA(nn.Module):#TransCNN_Plus(nn.Module):\n",
    "    def __init__(self, DEVICE):\n",
    "        super(FFA, self).__init__()\n",
    "        dim = 256\n",
    "        #self.config = config\n",
    "        self.patch_to_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = 4, p2 = 4),\n",
    "            nn.Linear(4*4*3, dim)\n",
    "        )\n",
    "        self.transformer_enc = TransformerEncoders(dim, nhead=2, num_encoder_layers=9, dim_feedforward=dim*2, activation='gelu', withCIA=True)\n",
    "        self.cnn_dec = CNNDecoder(256, 3, 'ln', 'lrelu', 'reflect')\n",
    "        \n",
    "        b = 3 #self.config.BATCH_SIZE\n",
    "        MODE=2\n",
    "        if MODE == 2:#self.config.MODE == 2:\n",
    "            b = 1\n",
    "        input_pos = PatchPositionEmbeddingSine(ksize=4, stride=4)\n",
    "        self.input_pos = input_pos.unsqueeze(0).repeat(b, 1, 1, 1).to(DEVICE)\n",
    "        self.input_pos = self.input_pos.flatten(2).permute(2, 0, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #inputs = F.interpolate(inputs, (256, 256), mode=\"bilinear\", align_corners=False)\n",
    "        patch_embedding = self.patch_to_embedding(inputs)\n",
    "        content = self.transformer_enc(patch_embedding.permute(1, 0, 2), src_pos=self.input_pos)\n",
    "        bs, L, C  = patch_embedding.size()\n",
    "        content = content.permute(1,2,0).view(bs, C, int(math.sqrt(L)), int(math.sqrt(L)))\n",
    "        output = self.cnn_dec(content)\n",
    "        #output = F.interpolate(output, (256, 768), mode=\"bilinear\", align_corners=False)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, norm, activ, pad_type):\n",
    "        super(CNNDecoder, self).__init__()\n",
    "        self.model = []\n",
    "        dim = input_dim\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            Conv2dBlock(dim, dim // 2, 3, 1, 1, norm=norm, activation=activ, pad_type=pad_type)\n",
    "        )\n",
    "        dim //= 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            Conv2dBlock(dim, dim // 2, 3, 1, 1, norm=norm, activation=activ, pad_type=pad_type)\n",
    "        )\n",
    "        self.conv3 = Conv2dBlock(dim//2, output_dim, 5, 1, 2, norm='none', activation='tanh', pad_type=pad_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        output = self.conv3(x2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self, input_dim ,output_dim, kernel_size, stride,\n",
    "                 padding=0, norm='none', activation='relu', pad_type='zero', groupcount=16):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "        self.use_bias = True\n",
    "        self.norm_type = norm\n",
    "        # initialize padding\n",
    "        if pad_type == 'reflect':\n",
    "            self.pad = nn.ReflectionPad2d(padding)\n",
    "        elif pad_type == 'replicate':\n",
    "            self.pad = nn.ReplicationPad2d(padding)\n",
    "        elif pad_type == 'zero':\n",
    "            self.pad = nn.ZeroPad2d(padding)\n",
    "        else:\n",
    "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
    "\n",
    "        # initialize normalization\n",
    "        norm_dim = output_dim\n",
    "        if norm == 'bn':\n",
    "            self.norm = nn.BatchNorm2d(norm_dim)\n",
    "        elif norm == 'in':\n",
    "            #self.norm = nn.InstanceNorm2d(norm_dim, track_running_stats=True)\n",
    "            self.norm = nn.InstanceNorm2d(norm_dim)\n",
    "        elif norm == 'ln':\n",
    "            self.norm = LayerNorm(norm_dim)\n",
    "        elif norm == 'adain':\n",
    "            self.norm = AdaptiveInstanceNorm2d(norm_dim)\n",
    "        elif norm == 'adain_ori':\n",
    "            self.norm = AdaptiveInstanceNorm2d_IN(norm_dim)\n",
    "        elif norm == 'remove_render':\n",
    "            self.norm = RemoveRender(norm_dim)\n",
    "        elif norm == 'grp':\n",
    "            self.norm = nn.GroupNorm(groupcount, norm_dim)\n",
    "        \n",
    "        elif norm == 'none' or norm == 'sn':\n",
    "            self.norm = None\n",
    "        else:\n",
    "            assert 0, \"Unsupported normalization: {}\".format(norm)\n",
    "\n",
    "        # initialize activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif activation == 'lrelu':\n",
    "            self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif activation == 'prelu':\n",
    "            self.activation = nn.PReLU()\n",
    "        elif activation == 'selu':\n",
    "            self.activation = nn.SELU(inplace=True)\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'none':\n",
    "            self.activation = None\n",
    "        else:\n",
    "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
    "\n",
    "        # initialize convolution\n",
    "        if norm == 'sn':\n",
    "            self.conv = SpectralNorm(nn.Conv2d(input_dim, output_dim, kernel_size, stride, bias=self.use_bias))\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride, bias=self.use_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(self.pad(x))\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, affine=True):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n",
    "            self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = [-1] + [1] * (x.dim() - 1)\n",
    "        # print(x.size())\n",
    "        if x.size(0) == 1:\n",
    "            # These two lines run much faster in pytorch 0.4 than the two lines listed below.\n",
    "            mean = x.view(-1).mean().view(*shape)\n",
    "            std = x.view(-1).std().view(*shape)\n",
    "        else:\n",
    "            mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
    "            std = x.view(x.size(0), -1).std(1).view(*shape)\n",
    "\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "\n",
    "        if self.affine:\n",
    "            shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60329449-5ae1-4eba-911b-23a435af666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFA(DEVICE).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77de45ae-86eb-455a-86f2-d0dd2766ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c81acd6-47be-4579-9476-f4719ee8476e",
   "metadata": {},
   "source": [
    "## Training\n",
    "Training is done by taking random timesteps and learning the noise. The test model was trained by overfitting on a single batch to check model functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a997086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, loader, n_epochs, optim, device, display=False, store_path=\"simplenet.pt\"):\n",
    "    mse = nn.MSELoss()\n",
    "    best_loss = float(\"inf\")\n",
    "    n_steps = 0\n",
    "    display=0\n",
    "    for epoch in tqdm(range(n_epochs), desc=f\"Training progress\", colour=\"#00ff00\"):\n",
    "        epoch_loss = 0.0\n",
    "        pbar = tqdm(train_data_loader)\n",
    "        display=0\n",
    "        for x0, y in pbar:#step, batch in enumerate(tqdm(loader, leave=False, desc=f\"Epoch {epoch + 1}/{n_epochs}\", colour=\"#005500\")):\n",
    "            # Loading data\n",
    "            #x0 = batch[0].to(device)\n",
    "            x0 = x0.to(DEVICE)\n",
    "            y =y.to(DEVICE)\n",
    "\n",
    "            # Getting model estimation of noise based on the images and the time-step\n",
    "            eta_theta = model(x0)\n",
    "\n",
    "            # Optimizing the MSE between the noise plugged and the predicted noise\n",
    "            loss = mse(eta_theta, y)#y\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            if epoch%2==0:\n",
    "                epoch1=epoch\n",
    "                grid2 = make_grid(eta_theta, normalize=True, value_range=(-1, 1), nrow=4)\n",
    "                save_image(grid2, f\"modelsimple512_{epoch1 // 1}.png\")\n",
    "                grid3 = make_grid(y, normalize=True, value_range=(-1, 1), nrow=4)\n",
    "                save_image(grid3, f\"tarsimple_{epoch1 // 1}.png\")\n",
    "                display=1\n",
    "                    \n",
    "\n",
    "            epoch_loss += loss.item() * len(x0) / len(loader.dataset)\n",
    "\n",
    "        # Display images generated at this epoch\n",
    "        #if epoch%2==0:\n",
    "        #    show_images(, f\"Images generated at epoch {epoch + 1}\")\n",
    "\n",
    "        log_string = f\"Loss at epoch {epoch + 1}: {epoch_loss:.3f}\"\n",
    "\n",
    "        # Storing the model\n",
    "        if best_loss > epoch_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), store_path)\n",
    "            log_string += \" --> Best model ever (stored)\"\n",
    "\n",
    "        print(log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30c187f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, title=\"\"):\n",
    "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
    "\n",
    "    # Converting images to CPU numpy arrays\n",
    "    if type(images) is torch.Tensor:\n",
    "        images = images.detach().cpu().numpy()\n",
    "\n",
    "    # Defining number of rows and columns\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    rows = int(len(images) ** (1 / 2))\n",
    "    cols = round(len(images) / rows)\n",
    "\n",
    "    # Populating figure with sub-plots\n",
    "    idx = 0\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            fig.add_subplot(rows, cols, idx + 1)\n",
    "\n",
    "            if idx < len(images):\n",
    "                plt.imshow(images[idx][0], cmap=\"gray\")\n",
    "                idx += 1\n",
    "    fig.suptitle(title, fontsize=30)\n",
    "\n",
    "    # Showing the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b365405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   0%|\u001b[38;2;0;255;0m                                \u001b[0m| 0/200 [00:00<?, ?it/s]\u001b[0m\n",
      "  0%|                                                   | 0/667 [00:00<?, ?it/s]\u001b[A\n",
      "Training progress:   0%|\u001b[38;2;0;255;0m                                \u001b[0m| 0/200 [00:00<?, ?it/s]\u001b[0m\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 23.66 GiB of which 315.56 MiB is free. Process 41026 has 18.56 GiB memory in use. Including non-PyTorch memory, this process has 3.94 GiB memory in use. Of the allocated memory 3.15 GiB is allocated by PyTorch, and 561.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(SEED)\n\u001b[1;32m     23\u001b[0m loader \u001b[38;5;241m=\u001b[39m train_data_loader\n\u001b[0;32m---> 25\u001b[0m training_loop(model, loader, EPOCHS, optim\u001b[38;5;241m=\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr), device\u001b[38;5;241m=\u001b[39mdevice, store_path\u001b[38;5;241m=\u001b[39mstore_path)\n",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, loader, n_epochs, optim, device, display, store_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m y \u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Getting model estimation of noise based on the images and the time-step\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m eta_theta \u001b[38;5;241m=\u001b[39m model(x0)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Optimizing the MSE between the noise plugged and the predicted noise\u001b[39;00m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse(eta_theta, y)\u001b[38;5;66;03m#y\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 64\u001b[0m, in \u001b[0;36mFFA.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m#inputs = F.interpolate(inputs, (256, 256), mode=\"bilinear\", align_corners=False)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     patch_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_to_embedding(inputs)\n\u001b[0;32m---> 64\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_enc(patch_embedding\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m), src_pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_pos)\n\u001b[1;32m     65\u001b[0m     bs, L, C  \u001b[38;5;241m=\u001b[39m patch_embedding\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     66\u001b[0m     content \u001b[38;5;241m=\u001b[39m content\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mview(bs, C, \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39msqrt(L)), \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39msqrt(L)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m, in \u001b[0;36mTransformerEncoders.forward\u001b[0;34m(self, src, src_key_padding_mask, src_pos)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, src_key_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, src_pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 32\u001b[0m     memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, pos\u001b[38;5;241m=\u001b[39msrc_pos, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memory\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, pos)\u001b[0m\n\u001b[1;32m     47\u001b[0m output \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 51\u001b[0m     output \u001b[38;5;241m=\u001b[39m layer(output, src_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[1;32m     52\u001b[0m                    src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask, pos\u001b[38;5;241m=\u001b[39mpos)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 118\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, pos)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_before:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_pre(src, src_mask, src_key_padding_mask, pos)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_post(src, src_mask, src_key_padding_mask, pos)\n",
      "Cell \u001b[0;32mIn[8], line 87\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward_post\u001b[0;34m(self, src, src_mask, src_key_padding_mask, pos)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_post\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     82\u001b[0m                  src,\n\u001b[1;32m     83\u001b[0m                  src_mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m                  src_key_padding_mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m                  pos: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     86\u001b[0m     q \u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_pos_embed(src, pos)\n\u001b[0;32m---> 87\u001b[0m     src2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_token_mixer(q, k, value\u001b[38;5;241m=\u001b[39msrc, attn_mask\u001b[38;5;241m=\u001b[39msrc_mask,\n\u001b[1;32m     88\u001b[0m                           key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwithCIA \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m         src2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator(src, src2)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1242\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[1;32m   1245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1246\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1247\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1248\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1249\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1250\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1251\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:5442\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5441\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m-> 5442\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m softmax(attn_output_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   5444\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m dropout(attn_output_weights, p\u001b[38;5;241m=\u001b[39mdropout_p)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1860\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 23.66 GiB of which 315.56 MiB is free. Process 41026 has 18.56 GiB memory in use. Including non-PyTorch memory, this process has 3.94 GiB memory in use. Of the allocated memory 3.15 GiB is allocated by PyTorch, and 561.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "store_path ='./samples/models/simplenet.pt'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_OUT_DIR = r\"./models\"\n",
    "SAMPLE_OUT_DIR = r\"./samples\"\n",
    "\n",
    "# Diffusion\n",
    "NOISE_STEPS = 200\n",
    "BETA_START = 1e-4\n",
    "BETA_END = 2e-2\n",
    "\n",
    "\n",
    "# Training\n",
    "lr = 1e-5\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "# Low effort reproducibility\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "loader = train_data_loader\n",
    "\n",
    "training_loop(model, loader, EPOCHS, optim=Adam(model.parameters(), lr), device=device, store_path=store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeaf9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4490a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977f299-30f0-4745-b5e1-bc128bfa39c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f09f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from matplotlib import cm as c\n",
    "\n",
    "#model = SimpleNet(3,3,128).to(DEVICE)\n",
    "#model.load_state_dict(torch.load(f\"{MODEL_OUT_DIR}/diff41689934001018.pt\"))\n",
    "model.load_state_dict(torch.load(f\"./samples/models/simplenet.pt\"))\n",
    "\n",
    "print(testing_data_loader)\n",
    "pbar = tqdm(testing_data_loader)\n",
    "i=1\n",
    "for x_0, y in pbar:\n",
    "    x_0 = x_0.to(DEVICE)\n",
    "    y =y.to(DEVICE)\n",
    "\n",
    "\n",
    "            # Getting model estimation of noise based on the images and the time-step\n",
    "    eta_theta = model(x_0)\n",
    "        \n",
    "    target = y\n",
    "        \n",
    "    target = target.type(torch.FloatTensor)#.cuda()\n",
    "    target = Variable(target)\n",
    "    #print(target.shape)\n",
    "    #print(eta_theta.shape)\n",
    "    \n",
    "    for im in range(0,4):\n",
    "        print(im)\n",
    "        grid = make_grid(eta_theta[im], normalize=True, value_range=(-1, 1), nrow=4)\n",
    "        save_image(grid, f\"{SAMPLE_OUT_DIR}/pre512_{im}{i // 1}.png\")\n",
    "        grid0 = make_grid(target[im], normalize=True, value_range=(-1, 1), nrow=4)\n",
    "        save_image(grid0, f\"{SAMPLE_OUT_DIR}/targ_{im}{i // 1}.png\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    i=i+1\n",
    "        \n",
    "\n",
    "print(f\"Training loop finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f7cc8-66d6-47ef-87ac-08261384eee5",
   "metadata": {},
   "source": [
    "## Loading\n",
    "Loading the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67480b88-7f73-4f8c-8c78-02f1512765cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SimpleNet(3,3,128).to(DEVICE)\n",
    "# model.load_state_dict(torch.load(f\"{MODEL_OUT_DIR}/1686675513310.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1949ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff4c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
